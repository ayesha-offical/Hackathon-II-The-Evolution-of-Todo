# Claude Code Workflow & SDD Integration

This document describes how Claude Code integrates with the Spec-Driven Development (SDD) workflow for the Todo In-Memory Python Console App. All code generation, testing, and commits follow strict SDD principles defined in `.specify/memory/constitution.md` (v1.1.0).

---

## Core Principles for Claude Code

Claude Code operates under these non-negotiable principles during Phase 1:

### 1. Spec-Driven Development Lifecycle

**All work follows this sequence**:

```
Specify → Plan → Tasks → Implement → Commit
```

- **No code without specifications** — Specifications are the source of truth
- **No features without tasks** — Tasks break down plans into actionable units
- **No manual code editing** — All code must be generated by Claude Code based on approved tasks
- **No scope creep** — Only implement what's in the approved specification

### 2. Code Traceability (Principle XIII)

All generated code **MUST** reference the Task ID that produced it:

```python
# [T-001] - Spec section: FR-001 (Task model with ID generation)
class Task:
    """Model for todo task with auto-generated ID."""
    id: str
    title: str
    description: str
    completed: bool
```

**In file headers**:
```python
"""
Module: Task model implementation
Task ID: T-001
Specification Reference: FR-001 - System MUST support task creation with metadata
"""
```

**Requirement**: Every code file must have a header comment linking to the originating Task ID.

### 3. Git Commit Conventions (Principle VIII)

Every commit **MUST** follow this format:

```
[T-###] Brief description of change

Optional detailed explanation of what was implemented and why.
References to specification sections or acceptance criteria.
```

**Example**:
```
[T-001] Implement Task model with ID generation and timestamps

Adds Task class to src/models.py with required attributes:
- id (auto-generated UUID)
- title (string, required)
- description (string, optional)
- completed (boolean, default False)
- created_at, updated_at (ISO timestamps)

Satisfies spec requirement FR-001: System MUST support task creation with metadata.
Tests passing: test_task_creation, test_task_validation (80% coverage maintained)
```

**Task ID Format**: T-001, T-002, T-003, etc. (from `specs/phase-1/tasks.md`)

### 4. Testing Discipline (Principle VII)

- **80% Minimum Code Coverage** — All code must contribute to this target
- **Test-First (TDD)**: Write tests first (RED), watch fail, implement (GREEN), refactor
- **Framework**: pytest + pytest-cov
- **Coverage Measurement**: `uv run pytest tests/ --cov=src --cov-report=term-missing`

Before committing code:
```bash
# Verify all tests pass with coverage
uv run pytest tests/ --cov=src --cov-report=term-missing
```

### 5. Dependency Management (Principle XIV)

**Pre-Approved Libraries**:
- ✅ `rich` — CLI formatting and output
- ✅ `pytest` — Testing framework
- ✅ `pytest-cov` — Code coverage measurement

**For new dependencies**:
1. Justify why pre-approved alternatives are insufficient
2. Require specification amendment (add to `specs/phase-1/spec.md`)
3. Install via UV only: `uv add <package>`

Never use `pip`, `conda`, or direct `requirements.txt` edits.

---

## Workflow Commands & Claude Code Integration

### Phase 1: Specification

**File**: `specs/phase-1/spec.md`

**Command**: Use `/sp.specify` to create or amend specifications

```bash
/sp.specify
# Prompts for feature description
# Generates spec.md with user stories, requirements, acceptance criteria
```

**Claude Code Role**: Generates specification document structure, user scenarios, functional requirements, and success criteria from user input.

**Output Sections**:
- User Scenarios & Testing (P1, P2, P3 prioritized stories)
- Functional Requirements (FR-001, FR-002, ...)
- Key Entities (data model outline)
- Success Criteria (measurable outcomes)
- **Code Quality & Coverage Targets** (80% minimum, per Principle VII)

**Review**: User must approve specification before proceeding to planning.

---

### Phase 2: Planning

**File**: `specs/phase-1/plan.md`

**Command**: Use `/sp.plan` to create implementation plan from specification

```bash
/sp.plan
# Reads approved spec.md
# Generates plan.md with architecture, structure, decisions
```

**Claude Code Role**: Analyzes specification to produce technical architecture, project structure, and design decisions.

**Output Sections**:
- Technical Context (Python 3.13+, UV, pytest, in-memory storage)
- **Constitution Check** (verify compliance with v1.1.0 principles)
- **Dependency Validation** (list pre-approved libraries used, flag new dependencies)
- Project Structure (src/, tests/, etc.)
- Complexity Tracking (justify any violations)

**Constitution Compliance Gates**:
- ✅ Python 3.13+ confirmed
- ✅ UV for all dependencies
- ✅ In-memory storage only (Phase 1)
- ✅ No manual coding policy
- ✅ SDD workflow enforced
- ✅ 80% coverage target
- ✅ Task ID traceability planned

**Review**: User must approve plan and constitution check before task generation.

---

### Phase 3: Task Generation

**File**: `specs/phase-1/tasks.md`

**Command**: Use `/sp.tasks` to break plan into executable tasks

```bash
/sp.tasks
# Reads approved spec.md and plan.md
# Generates tasks.md with granular, independently-testable units
```

**Claude Code Role**: Generates task list with explicit Task IDs, dependencies, and acceptance criteria.

**Task Format**:
```
[T-001] [P] [US1] Implement Task model in src/models.py
```

- **T-001**: Task ID (referenced in commits)
- **[P]**: Can run in parallel (optional)
- **US1**: User Story 1 (optional)
- **Description**: File paths, acceptance criteria, dependencies

**Task Structure Example**:

```markdown
## Phase 3: User Story 1 - Add Task (Priority: P1)

### Tests for User Story 1

- [ ] T-001 [US1] Write test_task_creation in tests/test_models.py
- [ ] T-002 [US1] Write test_task_validation in tests/test_models.py

### Implementation for User Story 1

- [ ] T-003 [US1] Implement Task model in src/models.py (depends on T-001, T-002)
- [ ] T-004 [US1] Implement TaskStorage in src/storage.py
- [ ] T-005 [US1] Implement add_task command in src/cli.py

```

**Coverage Requirements in Tasks**:
- All acceptance criteria must have test cases
- Each task must specify test file references
- Coverage target: 80% by task completion
- Use `[T-XXX] - Spec section: FR-001` format in comments

**Review**: User must approve task list before implementation begins.

---

### Phase 4: Implementation

**Command**: Use `/sp.implement` to execute all tasks

```bash
/sp.implement
# Reads approved spec.md, plan.md, tasks.md
# Generates code implementing each task
# Commits with [T-###] references
```

**Claude Code Role**: Implements each task while maintaining traceability and quality standards.

**For Each Task**:

1. **Read the task** from `tasks.md` including acceptance criteria
2. **Reference the specification** (`specs/phase-1/spec.md`) for requirements
3. **Write tests first** (TDD: RED) in appropriate test file
4. **Implement code** (GREEN) with Task ID comments
5. **Verify coverage** (`pytest --cov=src`)
6. **Commit with Task ID** (`[T-###] Description`)

**Code Generation Requirements**:

✅ Task ID in comments
```python
# [T-003] - Spec section: FR-001 (Task model)
class Task:
    """Task model with auto-generated ID."""
```

✅ File header with Task ID
```python
"""
Module: Task model implementation
Task ID: T-003
Specification Reference: FR-001 - System MUST support task creation
"""
```

✅ Type hints on all functions
```python
def create_task(title: str, description: str = "") -> Task:
    """Create a new task."""
    pass
```

✅ PEP 8 compliance (100 char max line length)

✅ Error handling (no silent failures)
```python
if not title or not title.strip():
    raise ValueError("Task title cannot be empty")
```

✅ Test coverage (80% minimum)
```bash
uv run pytest tests/ --cov=src --cov-report=term-missing
```

**Commit Format**:
```
[T-003] Implement Task model with ID generation and timestamps

Adds Task class to src/models.py with required attributes:
- id (auto-generated UUID)
- title (string, required)
- description (string, optional)
- completed (boolean, default False)
- created_at, updated_at (ISO timestamps)

Satisfies spec requirement FR-001: System MUST support task creation with metadata.
Tests passing: test_task_creation, test_task_validation
Coverage: 80%+ maintained across all modifications
```

---

## Pre-Implementation Checklist

Before Claude Code generates code:

### 1. Specification Phase
- [ ] `specs/phase-1/spec.md` exists and is approved
- [ ] All user stories have independent test scenarios
- [ ] All requirements are marked FR-001, FR-002, etc.
- [ ] Code Quality & Coverage Targets section completed (80% minimum)
- [ ] Edge cases documented

### 2. Planning Phase
- [ ] `specs/phase-1/plan.md` exists and is approved
- [ ] Constitution Check passes (all 14 principles verified)
- [ ] Dependency Validation completed (only pre-approved libs used)
- [ ] Project structure defined (src/, tests/ directories)
- [ ] No violations without documented justification

### 3. Task Generation Phase
- [ ] `specs/phase-1/tasks.md` exists and is approved
- [ ] All tasks have explicit Task IDs (T-001, T-002, ...)
- [ ] Each task references spec section (e.g., FR-001)
- [ ] All acceptance criteria listed
- [ ] Tests tasks come before implementation tasks
- [ ] Coverage requirements specified

### 4. Implementation Phase
- [ ] Specification, Plan, Tasks all approved
- [ ] Working directory clean (`git status` shows no uncommitted changes)
- [ ] Python 3.13+ available (`python --version`)
- [ ] UV installed and working (`uv --version`)
- [ ] All dependencies resolvable (`uv lock`)
- [ ] All tests run successfully (`uv run pytest tests/`)

---

## Handling Implementation Failures

If Claude Code generates incorrect code:

1. **Do NOT manually patch the code** — This violates the Zero Manual Coding Policy
2. **Instead**:
   - Identify the failing task (e.g., T-003)
   - Return to the specification/plan/task to clarify requirements
   - Amend the task with clearer acceptance criteria
   - Re-run `/sp.implement` for that task
3. **Update the specification chain**:
   - Specification amendment → Plan update → Task clarification → Re-implement

**Example**:
```
❌ WRONG: Edit src/models.py manually to fix test failures

✅ RIGHT:
1. Review task T-003 acceptance criteria
2. Update spec.md requirement FR-001 for clarity
3. Update plan.md architecture decision
4. Update task T-003 acceptance criteria
5. Re-run /sp.implement for T-003
```

---

## Code Review & Validation

After Claude Code commits code:

```bash
# Verify tests pass
uv run pytest tests/ -v

# Verify coverage meets 80% minimum
uv run pytest tests/ --cov=src --cov-report=term-missing

# Verify PEP 8 compliance (if linter configured)
# uv run flake8 src/ tests/

# View recent commits
git log --oneline -10
# Expected: [T-001] ..., [T-002] ..., [T-003] ...
```

**Before deploying Phase 1**:
- [ ] All 5 features implemented (Add, Delete, Update, View, Mark Complete)
- [ ] All tests passing (100%)
- [ ] Coverage ≥ 80% (`--cov-report=term-missing` shows green)
- [ ] All commits have Task ID format `[T-###]`
- [ ] README.md complete with setup instructions
- [ ] CLAUDE.md documents workflow
- [ ] specs/ folder contains spec.md, plan.md, tasks.md

---

## Amendment Workflow

To amend specifications, plans, or tasks during Phase 1:

### 1. Amendment Request
```
User: "The Add Task feature should also support tags"
```

### 2. Specification Amendment
```
/sp.specify
# Review current spec.md
# Identify requirement that needs amendment
# Update FR-001 to include: "System MUST support task tags (optional, comma-separated)"
# Append to Edge Cases: "What if user enters invalid tag format?"
# Update Code Quality section: New tests must cover tag parsing
```

### 3. Plan Amendment
```
/sp.plan
# Review spec changes
# Update plan.md:
#   - Data model: Add tags field to Task entity
#   - Dependencies: May need library for tag parsing (requires approval)
#   - Complexity: Minimal (single field addition)
# Re-check Constitution compliance
```

### 4. Task Amendment
```
/sp.tasks
# Review plan changes
# Create new tasks:
#   - T-XXX: Add tags field to Task model
#   - T-XXX: Implement tag parsing validation
#   - T-XXX: Update add_task CLI command to accept tags
# Mark dependent tasks that need updates (e.g., tests)
```

### 5. Implementation
```
/sp.implement
# Execute amended tasks only
# Commits will be [T-XXX] for new/updated tasks
```

---

## Quick Reference: Key Files

| File | Purpose | Claude Code Role |
|------|---------|-----------------|
| `.specify/memory/constitution.md` | Governance & principles (v1.1.0) | Reference & validate compliance |
| `specs/phase-1/spec.md` | Feature requirements & acceptance | Generate structure; user fills details |
| `specs/phase-1/plan.md` | Architecture & technical decisions | Generate options; user selects |
| `specs/phase-1/tasks.md` | Task breakdown with traceability | Generate list with Task IDs |
| `src/main.py` | CLI entry point | Implement per task |
| `src/models.py` | Task model with ID generation | Implement per task |
| `src/storage.py` | In-memory storage layer | Implement per task |
| `src/cli.py` | CLI commands (add, delete, update, view, complete) | Implement per task |
| `tests/*.py` | Test files (pytest) | Implement tests FIRST (TDD) |
| `pyproject.toml` | UV project config (managed by `uv add`) | Never edit manually |
| `uv.lock` | Locked dependencies (auto-generated) | Never edit manually |

---

## Troubleshooting

### Problem: Test coverage below 80%
```bash
# Run coverage report with missing lines
uv run pytest tests/ --cov=src --cov-report=term-missing

# Look for:
# - src/models.py, line 15-20 — missing
# - Add test case to test_models.py covering those lines
```

### Problem: Commit doesn't reference Task ID
```
# Wrong: "Implement Task model"
# Right: "[T-003] Implement Task model with ID generation and timestamps"

# Fix: Amend commit message
git commit --amend -m "[T-003] Implement Task model..."
```

### Problem: New dependency breaks pre-approved list
```
Error: Package 'xyz' not in pre-approved list

# Fix:
# 1. Update spec.md: Add justification under new requirement
# 2. Update plan.md: Add Dependency Validation section explaining why needed
# 3. Run: uv add xyz  (after amendment approved)
# 4. Commit: [T-XXX] Add xyz dependency (with amendment reference)
```

### Problem: Code doesn't have Task ID comments
```python
# Wrong:
class Task:
    pass

# Right:
# [T-003] - Spec section: FR-001 (Task model with ID generation)
class Task:
    pass
```

### Problem: Git log shows manual commits instead of Task IDs
```bash
# View recent commits
git log --oneline -10

# Expected: [T-001] ..., [T-002] ..., [T-003] ...
# Wrong: "Add Task model", "Fix bug", "Update storage"
```

---

## Phase 1 Completion Criteria

The Phase 1 MVP is complete when:

✅ **All 5 Features Implemented**
- [ ] Add Task
- [ ] View Task List
- [ ] Update Task
- [ ] Mark as Complete
- [ ] Delete Task

✅ **All Tests Pass**
- [ ] Unit tests (test_models.py, test_storage.py)
- [ ] Integration tests (test_cli.py)
- [ ] Acceptance tests (test_acceptance.py)
- [ ] Coverage ≥ 80%

✅ **Code Quality**
- [ ] PEP 8 compliant
- [ ] Type hints on all functions
- [ ] Error handling for edge cases
- [ ] All commits reference Task IDs

✅ **Documentation Complete**
- [ ] `specs/phase-1/spec.md` final
- [ ] `specs/phase-1/plan.md` final
- [ ] `specs/phase-1/tasks.md` final
- [ ] `README.md` with setup & usage
- [ ] `CLAUDE.md` (this file)

✅ **Repository Ready**
- [ ] All code in git with Task ID commits
- [ ] No uncommitted changes
- [ ] `.gitignore` configured
- [ ] Public repository with clear structure

---

## Next Steps (Phase 2+)

Phase 2 extends with:
- Persistent storage (SQLite or PostgreSQL)
- Web frontend (React or FastAPI + HTML)
- User authentication
- Intermediate/Advanced features

All Phase 2 work follows the same SDD workflow and constitutional principles.

---

## Support & Questions

- **Workflow questions**: Refer to `.specify/memory/constitution.md` (v1.1.0)
- **Specification questions**: See `specs/phase-1/spec.md`
- **Implementation help**: Check `/sp.implement` logs
- **Git issues**: Review commit format in "Git Commit Conventions" section above
